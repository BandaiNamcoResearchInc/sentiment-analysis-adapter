{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.5 64-bit ('pytorch_p36': conda)",
   "display_name": "Python 3.6.5 64-bit ('pytorch_p36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "67602bcda1a2979e98e24a2c7b4e81c48fa406727758eae6f9b97279c00e3467"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Quick demo for training a **sentiment-analysis** adapter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, install adapter-transformers from gitHub and other required modules for Japanese tokenizer.\n",
    "\n",
    "最初に、GitHubから`adapter-transformers`をインストールし、他にも日本語のトークナイザーに必要なモジュールをインストールします。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/adapter-hub/adapter-transformers.git\n",
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3==0.996.5\n",
    "!pip install unidic-lite\n",
    "!pip install toiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    GlueDataset,\n",
    "    GlueDataTrainingArguments,\n",
    "    AutoModelWithHeads,\n",
    "    AdapterType,\n",
    "    AdapterConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    EvalPrediction\n",
    ")\n",
    "\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    glue_compute_metrics,\n",
    "    glue_output_modes,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from toiro import datadownloader"
   ]
  },
  {
   "source": [
    "Currently only BERT, Roberta & XLM-Roberta are supported by adapter-transformers integration.\n",
    "\n",
    "Here, we load a pretrained model([Pretrained BERT from TOHOKU NLP LAB](https://www.nlp.ecei.tohoku.ac.jp/news-release/3284/)) and add a new SST-2 task adapter.\n",
    "\n",
    "[SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary) is a binary classification benmark on sentiment analysis in English, but we could customize our dataset to make it work.\n",
    "\n",
    "現在`adapter-transformers`でサポートされているのは、BERT、Roberta、XLM-Robertaのみです。\n",
    "\n",
    "ここでは、事前学習済みモデル（[Pretrained BERT from TOHOKU NLP LAB](https://www.nlp.ecei.tohoku.ac.jp/news-release/3284/)）をロードし、新たにSST-2タスクのアダプターを追加します。  \n",
    "\n",
    "[SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary)は英語での感情分析の2値分類用ベンチマークですが、データセットをカスタマイズすることで動かすことができました。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "task_name = \"sst-2\"\n",
    "adapter_config = \"pfeiffer\"\n",
    "set_seed(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = glue_tasks_num_labels[task_name]\n",
    "output_mode = glue_output_modes[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=config)\n",
    "model.add_adapter(task_name, AdapterType.text_task, config=adapter_config)"
   ]
  },
  {
   "source": [
    "We freeze parameters except for those of sst-2 adapter.\n",
    "\n",
    "Besides, we could also store the classification head as well as adapter weights for reproducibility.\n",
    "\n",
    "SST-2アダプター以外のパラメータをフリーズします。\n",
    "\n",
    "また、再現性のためにアダプターの重みと同様に、頭の分類層も保存することができます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter([task_name])\n",
    "model.set_active_adapters([task_name])"
   ]
  },
  {
   "source": [
    "Then, we download the Yahoo Movie Reviews dataset and dump it into local folder for training.\n",
    "\n",
    "The pretrained adapter was train with 12500 rows of data.\n",
    "\n",
    "It takes about 5 minutes/epoch to run on Colab GPU for every 100 rows of data.\n",
    "If you just want to go through a quick demo, you can choose a smaller number such as n=125.\n",
    "\n",
    "そして、Yahoo!映画のユーザレビューデータセットをダウンロードし、訓練のためにローカルフォルダにダンプします。\n",
    "\n",
    "事前学習済みのアダプターは12500行のデータで訓練しました。\n",
    "\n",
    "早くデモをしてみたい場合は、n=125のような小さい数を選択することができます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"yahoo_movie_reviews\"\n",
    "datadownloader.download_corpus(corpus)\n",
    "train_df, dev_df, test_df = datadownloader.load_corpus(corpus, n=125)\n",
    "\n",
    "train_df.columns = ['label','sentence']\n",
    "dev_df.columns = ['label','sentence']\n",
    "\n",
    "train_df = train_df[['sentence', 'label']]\n",
    "dev_df = dev_df[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "train_df.to_csv(os.path.join(data_path, \"train.tsv\"), sep = '\\t', index = False)\n",
    "dev_df.to_csv(os.path.join(data_path, \"dev.tsv\"), sep = '\\t', index = False)"
   ]
  },
  {
   "source": [
    "We would configure the training and data arguments for training and define metric for evaluation.\n",
    "\n",
    "訓練のために訓練引数とデータ引数を設定し、評価のためのメトリクスを定義します。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataTrainingArguments(\n",
    "    task_name = task_name, \n",
    "    data_dir = data_path, \n",
    "    max_seq_length = 128,\n",
    "    overwrite_cache = True)\n",
    "\n",
    "train_dataset = GlueDataset(\n",
    "    data_args,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "eval_dataset = GlueDataset(\n",
    "    data_args,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return glue_compute_metrics(data_args.task_name, preds, p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_path,\n",
    "    per_device_train_batch_size = 1,\n",
    "    learning_rate = 1e-4,\n",
    "    num_train_epochs = 3.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    "    do_save_adapters = True,\n",
    "    )"
   ]
  },
  {
   "source": [
    "Finally, we start training our adapter in 3 epochs.\n",
    "\n",
    "最後に3エポックでアダプターの訓練を開始します。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "source": [
    "In addition, we could also evaluate our adapter and export all model and adapters in local file. \n",
    "\n",
    "さらに、アダプターを評価し、すべてのモデルとアダプターをローカルファイルにエクスポートすることもできます。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "eval_datasets = [eval_dataset]\n",
    "for eval_dataset in eval_datasets:\n",
    "    eval_result = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    eval_results.update(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ]
}